---
layout: default
title: Methodology Assignment 4
---

Jessica Zhang (jez004@ucsd.edu)  
**Section:** A07  
**Mentor:** Rajeev Chhajer, Ryan Lingo

---

## Methodology Assignment 4 - Task 2

### 1. What is the most interesting topic covered in your domain this quarter?
The most interesting topic in my domain this quarter has been how prompts and contexts shape LLM explanations — not just the model architecture itself. I learned that even small framing differences (like asking for an “ELI5” vs. “college-level” answer) can dramatically change factual accuracy, clarity, and depth. This idea connects closely to how humans adapt explanations to different audiences, and it helped me think of LLMs more as communicators than static predictors.

### 2. Describe a potential investigation you would like to pursue for your Quarter 2 Project.
For Quarter 2, I’d like to expand my current evaluation pipeline into a larger-scale study on LLM explanation alignment across domains. Specifically, I want to test whether a model that explains AI and statistics concepts well also maintains explanation quality in other technical areas, like biology or economics.

### 3. What is a potential change you’d make to the approach taken in your current Quarter 1 Project?
In Quarter 1, we focused on single-concept explanations and static scoring. For Q2, our team would change the approach by making the evaluation context-aware and iterative—allowing the model to refine its explanation after receiving feedback from either a human or another AI judge. This would make the system more dynamic and simulate real teaching or tutoring scenarios, where explanations evolve based on misunderstanding or feedback.

### 4. What other techniques would you be interested in using in your project?
Prompt optimization / Chain-of-thought prompting to improve reasoning consistency, and Langfuse for visualization of the entire evaluation process.
